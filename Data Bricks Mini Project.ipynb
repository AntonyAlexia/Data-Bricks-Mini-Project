{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28b603d2-b503-4f91-a103-fb77c23ff09b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files split and stored in: /dbfs/mnt/raw/sales_split\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SplitCSVFiles\").getOrCreate()\n",
    "\n",
    "sales_df = spark.read.csv(\"/Volumes/mini_project_12/mini_/data/sales.csv\", header=True, inferSchema=True)\n",
    "\n",
    "output_path = \"/dbfs/mnt/raw/sales_split\"\n",
    "sales_df.repartition(10).write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "print(\"CSV files split and stored in:\", output_path)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"CUST_ID\", StringType(), True),\n",
    "    StructField(\"AMOUNT_SOLD\", IntegerType(), True),\n",
    "    StructField(\"source_file\", StringType(), True),\n",
    "    StructField(\"loaded_ts\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze\")\n",
    "\n",
    "def load_to_bronze_layer(file_path, table_name):\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    df = df.withColumn(\"source_file\", input_file_name()) \\\n",
    "           .withColumn(\"loaded_ts\", current_timestamp())\n",
    "\n",
    "    df.write.format(\"delta\").mode(\"append\").partitionBy(\"loaded_ts\").saveAsTable(f\"bronze.{table_name}\")\n",
    "\n",
    "load_to_bronze_layer(\"/Volumes/mini_project_12/mini_/data/sales.csv\", \"sales_stg\")\n",
    "\n",
    "data_files = [\n",
    "    \"/Volumes/mini_project_12/mini_/data/costs.csv\",\n",
    "    \"/Volumes/mini_project_12/mini_/data/customers.csv\",\n",
    "    \"/Volumes/mini_project_12/mini_/data/promotions.csv\",\n",
    "    \"/Volumes/mini_project_12/mini_/data/supplementary_demographics.csv\",\n",
    "    \"/Volumes/mini_project_12/mini_/data/times.csv\"\n",
    "]\n",
    "\n",
    "data_table_names = [\n",
    "    \"cost_stg\",\n",
    "    \"customer_stg\",\n",
    "    \"promotions_stg\",\n",
    "    \"supplementary_demographics_stg\",\n",
    "    \"times_stg\"\n",
    "]\n",
    "\n",
    "for file_path, table_name in zip(data_files, data_table_names):\n",
    "    load_to_bronze_layer(file_path, table_name)\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS silver\")\n",
    "\n",
    "def create_silver_table(source_table, target_table, primary_key):\n",
    "    df = spark.sql(f\"SELECT * FROM bronze.{source_table}\")\n",
    "\n",
    "    window_spec = Window.partitionBy(primary_key).orderBy(\"loaded_ts\")\n",
    "    df = df.withColumn(\"rank\", row_number().over(window_spec))\n",
    "    df = df.filter(\"rank = 1\").drop(\"rank\")\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"silver.{target_table}\")\n",
    "\n",
    "create_silver_table(\"customer_stg\", \"customers_edw\", \"CUST_ID\")\n",
    "\n",
    "edw_mappings = [\n",
    "    (\"sales_stg\", \"sales_edw\", \"PROD_ID\"),\n",
    "    (\"promotions_stg\", \"promotions_edw\", \"PROMO_ID\"),\n",
    "    (\"supplementary_demographics_stg\", \"supplementary_demographics_edw\", \"CUST_ID\"),\n",
    "    (\"times_stg\", \"times_edw\", \"TIME_ID\"),\n",
    "    (\"cost_stg\", \"cost_edw\", \"PROD_ID\")\n",
    "]\n",
    "\n",
    "for source, target, key in edw_mappings:\n",
    "    create_silver_table(source, target, key)\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold\")\n",
    "\n",
    "def create_high_priority_view():\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        CONCAT(c.CUST_FIRST_NAME, ' ', c.CUST_LAST_NAME) AS full_name,\n",
    "        c.CUST_EMAIL AS email,\n",
    "        c.CUST_ID,\n",
    "        SUM(s.AMOUNT_SOLD) AS total_amount\n",
    "    FROM silver.customers_edw AS c\n",
    "    JOIN silver.sales_edw AS s\n",
    "    ON c.CUST_ID = s.CUST_ID\n",
    "    GROUP BY c.CUST_FIRST_NAME, c.CUST_LAST_NAME, c.CUST_EMAIL, c.CUST_ID\n",
    "    HAVING SUM(s.AMOUNT_SOLD) > 1500\n",
    "    \"\"\"\n",
    "    \n",
    "    high_priority_df = spark.sql(query)\n",
    "\n",
    "    spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold\")\n",
    "\n",
    "    high_priority_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold.high_priority_customer\")\n",
    "\n",
    "create_high_priority_view()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Bricks Mini Project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}